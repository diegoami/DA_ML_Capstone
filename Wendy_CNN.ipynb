{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in required libraries, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SageMaker Resources\n",
    "\n",
    "The below cell stores the SageMaker session and role (for creating estimators and models), and creates a default S3 bucket. After creating this bucket, you can upload any locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# default S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-18 21:09:16--  https://da-youtube-ml.s3.eu-central-1.amazonaws.com/wendy-cnn/frames/wendy_cnn_frames_data.zip\n",
      "Resolving da-youtube-ml.s3.eu-central-1.amazonaws.com (da-youtube-ml.s3.eu-central-1.amazonaws.com)... 52.219.75.180\n",
      "Connecting to da-youtube-ml.s3.eu-central-1.amazonaws.com (da-youtube-ml.s3.eu-central-1.amazonaws.com)|52.219.75.180|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3406848357 (3.2G) [application/zip]\n",
      "Saving to: ‘wendy_cnn_frames_data.zip’\n",
      "\n",
      "wendy_cnn_frames_da 100%[===================>]   3.17G  91.9MB/s    in 34s     \n",
      "\n",
      "2020-10-18 21:09:50 (95.0 MB/s) - ‘wendy_cnn_frames_data.zip’ saved [3406848357/3406848357]\n",
      "\n",
      "s3://sagemaker-eu-central-1-283211002347/cnn-wendy\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://da-youtube-ml.s3.eu-central-1.amazonaws.com/wendy-cnn/frames/wendy_cnn_frames_data.zip\n",
    "!unzip -qq -n wendy_cnn_frames_data.zip -d wendy_cnn_frames_data \n",
    "!rm wendy_cnn_frames_data.zip\n",
    "prefix='cnn-wendy'\n",
    "\n",
    "# upload to S3\n",
    "input_data = sagemaker_session.upload_data(path='wendy_cnn_frames_data', bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The next cell runs the above function to create a `train.csv` file in a specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell runs the above function to create a `train.csv` file in a specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that you've uploaded the data, by printing the contents of the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import subprocess as sb \n",
    "#import sys \n",
    "\n",
    "#sb.call([sys.executable, \"-m\", \"pip\", \"install\", 'scikit-learn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='letsplay_classifier', # this should be just \"source\" for your code\n",
    "                    role=role,\n",
    "                    framework_version='1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.8xlarge',\n",
    "                    train_volume_size = 10,\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'img-width': 128,\n",
    "                        'img-height': 72,\n",
    "                        'batch-size': 16,\n",
    "                        'epochs': 10\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Estimator\n",
    "\n",
    "After instantiating your estimator, train it with a call to `.fit()`. The `train.py` file explicitly loads in `.csv` data, so you do not need to convert the input data to any other format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 22:44:50 Starting - Starting the training job...\n",
      "2020-10-18 22:44:52 Starting - Launching requested ML instances.........\n",
      "2020-10-18 22:46:46 Starting - Preparing the instances for training.........\n",
      "2020-10-18 22:48:08 Downloading - Downloading input data........."
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Trained Model\n",
    "\n",
    "PyTorch models do not automatically come with `.predict()` functions attached (as many Scikit-learn models do, for example) and you may have noticed that you've been give a `predict.py` file. This file is responsible for loading a trained model and applying it to passed in, numpy data. When you created a PyTorch estimator, you specified where the training script, `train.py` was located. \n",
    "\n",
    "> How can we tell a PyTorch model where the `predict.py` file is?\n",
    "\n",
    "Before you can deploy this custom PyTorch model, you have to take one more step: creating a `PyTorchModel`. In earlier exercises you could see that a call to `.deploy()` created both a **model** and an **endpoint**, but for PyTorch models, these steps have to be separate.\n",
    "\n",
    "### EXERCISE: Instantiate a `PyTorchModel`\n",
    "\n",
    "You can create a `PyTorchModel` (different that a PyTorch estimator) from your trained, estimator attributes. This model is responsible for knowing how to execute a specific `predict.py` script. And this model is what you'll deploy to create an endpoint.\n",
    "\n",
    "#### Model Parameters\n",
    "\n",
    "To instantiate a `PyTorchModel`, ([documentation, here](https://sagemaker.readthedocs.io/en/stable/sagemaker.pytorch.html#sagemaker.pytorch.model.PyTorchModel)) you pass in the same arguments as your PyTorch estimator, with a few additions/modifications:\n",
    "* **model_data**: The trained `model.tar.gz` file created by your estimator, which can be accessed as `estimator.model_data`.\n",
    "* **entry_point**: This time, this is the path to the Python script SageMaker runs for **prediction** rather than training, `predict.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='1.0',\n",
    "                     entry_point='predict_fromfile.py',\n",
    "                     source_dir='letsplay_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Deploy the trained model\n",
    "\n",
    "Deploy your model to create a predictor. We'll use this to make predictions on our test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2019-03-12-03-24-09-384\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2019-03-12-03-24-09-384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------!CPU times: user 591 ms, sys: 62.9 ms, total: 654 ms\n",
      "Wall time: 7min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_fromfile import model_fn, predict_fn, input_fn, output_fn\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "index = 0\n",
    "loss_test = 0\n",
    "acc_test = 0\n",
    "count = 0\n",
    "cur_data_dir = 'wendy_cnn_frames_data'\n",
    "dirs = sorted(os.listdir(cur_data_dir))\n",
    "for dir in dirs:\n",
    "    labels = torch.empty(1, dtype=int)\n",
    "    labels[0] = index\n",
    "    print(labels)\n",
    "    curr_img_dir = os.path.join(args.data_dir, dir)\n",
    "    images = os.listdir(curr_img_dir)\n",
    "    for image in images:\n",
    "        curr_img = os.path.join(curr_img_dir, image)\n",
    "\n",
    "        with open(curr_img, 'rb') as f:\n",
    "            image_data = Image.open(f)\n",
    "           \n",
    "            prediction_from_ep = predictor.predict(image_data)  \n",
    "            prediction = torch.FloatTensor(prediction_from_ep).unsqueeze(0)\n",
    "\n",
    "            _, preds = torch.max(prediction.data, 1)\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            loss_test += loss.data\n",
    "            acc_test += torch.sum(preds == labels.data)\n",
    "            count += 1\n",
    "            avg_loss = torch.true_divide(loss_test, count)\n",
    "            avg_acc = torch.true_divide(acc_test, count)\n",
    "            print(\"{} processed\".format(count))\n",
    "            print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "            print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating Your Model\n",
    "\n",
    "Once your model is deployed, you can see how it performs when applied to the test data.\n",
    "\n",
    "The provided function below, takes in a deployed predictor, some test features and labels, and returns a dictionary of metrics; calculating false negatives and positives as well as recall, precision, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from letsplay_classifiers.train import get_data_loaders\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def eval_predictor(predictor, dataloaders):\n",
    "    \n",
    "    test_batches = len(dataloaders['val'])\n",
    "    print(\"Evaluating model\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    for i, data in enumerate(dataloaders['val']):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "\n",
    "dataloaders, dataset_sizes, class_names = get_data_loaders(\n",
    "    img_dir='wendy-cnn-data', 256, 256, 8 )\n",
    "\n",
    "eval_predictor(predictor) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results\n",
    "\n",
    "The cell below runs the `evaluate` function. \n",
    "\n",
    "The code assumes that you have a defined `predictor` and `X_test` and `Y_test` from previously-run cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions  0.0  1.0\n",
      "actuals              \n",
      "0             53   18\n",
      "1             11   68\n",
      "\n",
      "Recall:     0.861\n",
      "Precision:  0.791\n",
      "Accuracy:   0.807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get metrics for custom predictor\n",
    "metrics = evaluate(predictor, X_test, Y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Finally, I've add a convenience function to delete prediction endpoints after we're done with them. And if you're done evaluating the model, you should delete your model endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a predictor endpoint as input\n",
    "# And deletes the endpoint by name\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted sagemaker-pytorch-2019-03-12-03-24-09-384\n"
     ]
    }
   ],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleanup!\n",
    "\n",
    "* Double check that you have deleted all your endpoints.\n",
    "* I'd also suggest manually deleting your S3 bucket, models, and endpoint configurations directly from your AWS console.\n",
    "\n",
    "You can find thorough cleanup instructions, [in the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, you saw how to train and deploy a custom, PyTorch model in SageMaker. SageMaker has many built-in models that are useful for common clustering and classification tasks, but it is useful to know how to create custom, deep learning models that are flexible enough to learn from a variety of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
