{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in required libraries, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SageMaker Resources\n",
    "\n",
    "The below cell stores the SageMaker session and role (for creating estimators and models), and creates a default S3 bucket. After creating this bucket, locally stored data can be uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# default S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix='cnn-wendy-data-4b'\n",
    "prefix_output='cnn-wendy-model-4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we retrieve the dataset of images and we upload it to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nc https://da-youtube-ml.s3.eu-central-1.amazonaws.com/wendy-cnn/frames/wendy_cnn_frames_data_4.zip\n",
    "#!unzip -qq -n wendy_cnn_frames_data_4.zip -d wendy_cnn_frames_data_4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to cnn-wendy-data-4b\n",
      "Data uploaded to s3://sagemaker-eu-central-1-283211002347/cnn-wendy-data-4b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# upload to S3. Skip if already uploaded. This can take a while.\n",
    "print('Uploading data to {}'.format(prefix))\n",
    "input_data = sagemaker_session.upload_data(path='wendy_cnn_frames_data_4b', bucket=bucket, key_prefix=prefix)\n",
    "print('Data uploaded to {}'.format(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to input data can be written down here, if known\n",
    "input_data='s3://sagemaker-eu-central-1-283211002347/cnn-wendy-data-4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading images to S3, we can define and train the estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path for models is s3://sagemaker-eu-central-1-283211002347/cnn-wendy-model-4b\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix_output)\n",
    "print('Output path for models is {}'.format(output_path))\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='letsplay_classifier',\n",
    "                    role=role,\n",
    "                    framework_version='1.6',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'img-width': 320,\n",
    "                        'img-height': 180,\n",
    "                        'batch-size': 16,\n",
    "                        'layer-cfg': 'B',\n",
    "                        'epochs': 1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Estimator\n",
    "\n",
    "After instantiating the estimator, we train it with a call to `.fit()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-30 05:05:11 Starting - Starting the training job...\n",
      "2020-10-30 05:05:13 Starting - Launching requested ML instances......\n",
      "2020-10-30 05:06:21 Starting - Preparing the instances for training......\n",
      "2020-10-30 05:07:37 Downloading - Downloading input data...............\n",
      "2020-10-30 05:10:11 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-10-30 05:10:11,819 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-10-30 05:10:11,842 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-10-30 05:10:14,894 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-10-30 05:10:15,283 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.0)\u001b[0m\n",
      "\u001b[34mCollecting torchdata==0.2.0\n",
      "  Downloading torchdata-0.2.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 2)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 4)) (8.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torchdata\u001b[0m\n",
      "\u001b[34mSuccessfully installed torchdata-0.2.0\u001b[0m\n",
      "\u001b[34m2020-10-30 05:10:17,033 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"layer-cfg\": \"B\",\n",
      "        \"img-width\": 320,\n",
      "        \"epochs\": 1,\n",
      "        \"img-height\": 180\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-10-30-05-05-11-187\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-283211002347/pytorch-training-2020-10-30-05-05-11-187/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"epochs\":1,\"img-height\":180,\"img-width\":320,\"layer-cfg\":\"B\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-283211002347/pytorch-training-2020-10-30-05-05-11-187/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"epochs\":1,\"img-height\":180,\"img-width\":320,\"layer-cfg\":\"B\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-10-30-05-05-11-187\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-283211002347/pytorch-training-2020-10-30-05-05-11-187/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--epochs\",\"1\",\"--img-height\",\"180\",\"--img-width\",\"320\",\"--layer-cfg\",\"B\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_LAYER-CFG=B\u001b[0m\n",
      "\u001b[34mSM_HP_IMG-WIDTH=320\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_IMG-HEIGHT=180\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --batch-size 16 --epochs 1 --img-height 180 --img-width 320 --layer-cfg B\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mData Dir: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mModel Dir: /opt/ml/model\u001b[0m\n",
      "\u001b[34mEpoch 0/1\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTraining batch 0/789\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.095 algo-1:32 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.095 algo-1:32 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.096 algo-1:32 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.096 algo-1:32 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.117 algo-1:32 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-10-30 05:10:26.117 algo-1:32 INFO hook.py:459] Hook is writing from the hook with pid: 32\n",
      "\u001b[0m\n",
      "\u001b[34mTraining batch 100/789\u001b[0m\n",
      "\u001b[34mTraining batch 200/789\u001b[0m\n",
      "\u001b[34mTraining batch 300/789\u001b[0m\n",
      "\u001b[34mTraining batch 400/789\u001b[0m\n",
      "\u001b[34mTraining batch 500/789\u001b[0m\n",
      "\u001b[34mTraining batch 600/789\u001b[0m\n",
      "\u001b[34mTraining batch 700/789\u001b[0m\n",
      "\u001b[34mValidation batch 0/255\u001b[0m\n",
      "\u001b[34mValidation batch 100/255\u001b[0m\n",
      "\u001b[34mValidation batch 200/255\u001b[0m\n",
      "\u001b[34mEpoch 0 result: \u001b[0m\n",
      "\u001b[34mAvg loss (train): 0.0210\u001b[0m\n",
      "\u001b[34mAvg acc (train): 0.8921\u001b[0m\n",
      "\u001b[34mAvg loss (val): 0.0108\u001b[0m\n",
      "\u001b[34mAvg acc (val): 0.9568\u001b[0m\n",
      "\u001b[34m----------\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed in 9m 54s\u001b[0m\n",
      "\u001b[34mBest acc: 0.9568\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mEvaluating model\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\n",
      "2020-10-30 05:21:12 Uploading - Uploading generated training model\u001b[34m#015Test batch 0/170#015Test batch 100/170\u001b[0m\n",
      "\u001b[34mEvaluation completed in 0m 47s\u001b[0m\n",
      "\u001b[34mAvg loss (test): 0.0116\u001b[0m\n",
      "\u001b[34mAvg acc (test): 0.9496\u001b[0m\n",
      "\u001b[34m----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       395\n",
      "           1       0.63      0.95      0.76        56\n",
      "           2       0.99      0.96      0.97      1850\n",
      "           3       0.78      0.81      0.79        62\n",
      "           4       0.91      0.91      0.91       354\n",
      "\n",
      "    accuracy                           0.95      2717\n",
      "   macro avg       0.84      0.91      0.87      2717\u001b[0m\n",
      "\u001b[34mweighted avg       0.95      0.95      0.95      2717\n",
      "\u001b[0m\n",
      "\u001b[34m[[ 371    6    6    1   11]\n",
      " [   0   53    1    1    1]\n",
      " [  27    8 1783   12   20]\n",
      " [   1   11    0   50    0]\n",
      " [   6    6   19    0  323]]\u001b[0m\n",
      "\u001b[34m2020-10-30 05:21:08,916 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-10-30 05:22:25 Completed - Training job completed\n",
      "Training seconds: 888\n",
      "Billable seconds: 888\n",
      "CPU times: user 2.37 s, sys: 98.8 ms, total: 2.47 s\n",
      "Wall time: 17min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-283211002347/cnn-wendy-model-4b/pytorch-training-2020-10-30-05-05-11-187/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(estimator.model_data)\n",
    "model_data = estimator.model_data\n",
    "#model_data = 's3://sagemaker-eu-central-1-283211002347/cnn-wendy-model-2b/pytorch-training-2020-10-26-00-49-31-414/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data = 's3://sagemaker-eu-central-1-283211002347/cnn-wendy-model-2b/pytorch-training-2020-10-26-00-49-31-414/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a model that can predict the class of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the trained model\n",
    "\n",
    "We deploy our model to create a predictor. We'll use this to make predictions on our data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                     role = role,\n",
    "                     framework_version='1.6',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='letsplay_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------!CPU times: user 37.4 s, sys: 4.43 s, total: 41.8 s\n",
      "Wall time: 11min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "              \n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-inference-2020-10-30-05-23-42-751\n"
     ]
    }
   ],
   "source": [
    "# the endpoint where the predictor is located\n",
    "endpoint_name = predictor.endpoint\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = 'wendy_cnn_frames_data_4b'\n",
    "    \n",
    "dirs = [s for s in sorted(os.listdir(data_dir)) if os.path.isdir(os.path.join(data_dir, s))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 processed up to 2500\n",
      "10000 processed up to 5000\n",
      "15000 processed up to 7500\n",
      "20000 processed up to 10000\n",
      "25000 processed up to 12500\n",
      "30000 processed up to 15000\n",
      "35000 processed up to 17500\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from six import BytesIO\n",
    "from PIL import Image\n",
    "label_index = 0\n",
    "images_total, images_processed = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for dir in dirs:\n",
    "    curr_img_dir = os.path.join(data_dir, dir)\n",
    "    images = os.listdir(curr_img_dir)\n",
    "\n",
    "    # scanning all images belonging to a label\n",
    "    for image in images:\n",
    "        curr_img = os.path.join(curr_img_dir, image)\n",
    "        images_total += 1\n",
    "\n",
    "    \n",
    "        images_processed += 1\n",
    "\n",
    "        with open(curr_img, 'rb') as image_file:\n",
    "            # retrive most likely category from predictor\n",
    "            image = Image.open(image_file)\n",
    "            data = np.asarray(image)\n",
    "            \n",
    "            output = predictor.predict(data)\n",
    "            output_sv = output[0]\n",
    "            pred_index = np.argmax(output_sv)\n",
    "\n",
    "            images_processed += 1\n",
    "            y_true.append(label_index)\n",
    "            y_pred.append(pred_index)\n",
    "\n",
    "            if (images_processed % 5000 == 0):\n",
    "                print(\"{} processed up to {}\".format(images_processed, images_total))\n",
    "    label_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is deployed, we check how the predictor performs on our full dataset,\n",
    "ensuring that the predictions make sense. We produce a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#endpoint_name='pytorch-inference-2020-10-26-04-51-38-837'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      2822\n",
      "           1       0.67      0.92      0.77       404\n",
      "           2       0.98      0.96      0.97     13207\n",
      "           3       0.77      0.84      0.80       440\n",
      "           4       0.92      0.92      0.92      2528\n",
      "\n",
      "    accuracy                           0.95     19401\n",
      "   macro avg       0.85      0.92      0.88     19401\n",
      "weighted avg       0.96      0.95      0.95     19401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2665,    26,    48,    29,    54],\n",
       "       [    0,   370,     5,    26,     3],\n",
       "       [  210,    67, 12738,    58,   134],\n",
       "       [    9,    59,     1,   371,     0],\n",
       "       [   28,    30,   146,     0,  2324]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-25 15:59:24--  https://da-youtube-ml.s3.eu-central-1.amazonaws.com/wendy-cnn/frames/wendy_cnn_frames_E67.zip\n",
      "Resolving da-youtube-ml.s3.eu-central-1.amazonaws.com (da-youtube-ml.s3.eu-central-1.amazonaws.com)... 52.219.75.88\n",
      "Connecting to da-youtube-ml.s3.eu-central-1.amazonaws.com (da-youtube-ml.s3.eu-central-1.amazonaws.com)|52.219.75.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 76964860 (73M) [application/zip]\n",
      "Saving to: ‘wendy_cnn_frames_E67.zip’\n",
      "\n",
      "wendy_cnn_frames_E6 100%[===================>]  73.40M  97.8MB/s    in 0.8s    \n",
      "\n",
      "2020-10-25 15:59:25 (97.8 MB/s) - ‘wendy_cnn_frames_E67.zip’ saved [76964860/76964860]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://da-youtube-ml.s3.eu-central-1.amazonaws.com/wendy-cnn/frames/wendy_cnn_frames_E67.zip\n",
    "!unzip -qq -n wendy_cnn_frames_E67.zip -d wendy_cnn_frames_E67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:04-28:04 | Battle : 11% , Hideout : 43% , Other : 7% , Siege : 36% \n",
      "42:48-42:52 | Other : 35% , Tournament : 65% \n",
      "43:44-43:46 | ????? \n",
      "46:24-47:44 | Battle : 38% , Other : 12% , Tournament : 49% \n",
      "51:00-51:04 | Other : 30% , Tournament : 70% \n",
      "52:30-53:02 | Battle : 56% , Hideout : 30% , Other : 12% \n",
      "54:36-56:00 | Battle : 57% , Other : 11% , Tournament : 31% \n",
      "01:03:52-01:05:42 | Battle : 62% , Other : 10% , Tournament : 26% \n",
      "01:14:00-01:16:36 | Battle : 19% , Other : 7% , Tournament : 72% \n",
      "01:17:50-01:19:16 | Battle : 30% , Other : 9% , Tournament : 59% \n",
      "01:33:12-01:34:22 | Battle : 94% \n",
      "01:38:16-01:43:50 | Battle : 74% , Other : 8% , Tournament : 17% \n",
      "01:43:54-01:46:06 | Battle : 72% , Other : 10% , Tournament : 16% \n",
      "01:49:00-01:50:38 | Battle : 94% \n",
      "01:50:48-01:53:32 | Battle : 90% , Other : 8% \n",
      "01:55:52-01:57:46 | Battle : 92% \n"
     ]
    }
   ],
   "source": [
    "from letsplay_classifier.interval.predict_intervals_endpoint import evaluate\n",
    "evaluate(endpoint_name, 'wendy_cnn_frames_E67/E67', class_names= ['Battle', 'Hideout', 'Other', 'Siege', 'Tournament'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Finally, I've added a convenience function to delete prediction endpoints after we're done with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a predictor endpoint as input\n",
    "# And deletes the endpoint by name\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=endpoint_name)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d51d1652315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# delete the predictor endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
